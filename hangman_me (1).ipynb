{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Strategy Description: Transformer Model for Masked Character Prediction with Cross Entropy Loss\n",
        "\n",
        "#### Overview\n",
        "The provided code defines a Transformer-based approach to predict masked characters in words. This character-level sequence-to-sequence task involves masking certain characters in a word and training the model to predict the original characters at those masked positions. The model uses Cross Entropy Loss for optimization.\n",
        "\n",
        "#### Components and Workflow\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - **Reading Words**: Words are read from a text file where each word is on a separate line.\n",
        "   - **Dataset Class**: A custom `CharDataset` class is defined to handle the character-level dataset. Each word is randomly masked at different positions, and both the masked word and the original word are converted to sequences of indices.\n",
        "\n",
        "2. **DataLoader and Collate Function**:\n",
        "   - **DataLoader**: A PyTorch `DataLoader` is used to handle batching and shuffling of the dataset.\n",
        "   - **Collate Function**: A custom `collate_fn` pads the sequences to a maximum length, ensuring uniform input dimensions for the model.\n",
        "\n",
        "3. **Positional Encoding**:\n",
        "   - **Positional Encoding Module**: This module adds positional information to the token embeddings to help the Transformer model capture the order of the characters in the sequences.\n",
        "\n",
        "4. **Transformer Model**:\n",
        "   - **Model Architecture**: The `TransformerModel` class defines the Transformer architecture, including an embedding layer, positional encoding, Transformer layers (with specified number of encoder and decoder layers, attention heads, and feedforward dimensions), and a final linear layer to predict the character logits.\n",
        "   - **Hyperparameters**: The model is initialized with specific hyperparameters, such as `d_model` (embedding dimension), number of attention heads (`nhead`), number of encoder and decoder layers, and feedforward dimension (`dim_feedforward`).\n",
        "\n",
        "5. **Training Loop**:\n",
        "   - **Cross Entropy Loss**: The loss function used is `nn.CrossEntropyLoss`, which is suitable for multi-class classification tasks. It compares the predicted character probabilities with the actual characters (targets) and computes the loss.\n",
        "   - **Optimization**: The optimizer used is Adam with a specified learning rate.\n",
        "   - **Training Process**: The model is trained for a number of epochs, where for each batch, the masked words are fed into the model, predictions are made, the loss is computed, and the model parameters are updated via backpropagation.\n",
        "\n",
        "6. **Decoding and Inference**:\n",
        "   - **Greedy Decoding**: A simple greedy decoding strategy is used to iteratively fill in the masked characters in the word during inference. The model predicts the character with the highest probability for each masked position.\n",
        "   - **Probability Analysis**: During inference, the predicted probabilities are analyzed to provide a list of possible characters sorted by their predicted likelihood.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5FjQeXS5rkB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Read words from a .txt file\n",
        "def read_words_from_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        words = file.readlines()\n",
        "    words = [word.strip() for word in words]\n",
        "    return words\n",
        "\n",
        "# Example file path\n",
        "file_path = \"/content/words_250000_train.txt\"  # Ensure this file exists with one word per line\n",
        "words = read_words_from_file(file_path)\n",
        "# Define a character-level dataset\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, words, mask_token='_'):\n",
        "        self.words = words\n",
        "        self.mask_token = mask_token\n",
        "        self.chars = list(\"abcdefghijklmnopqrstuvwxyz_\")\n",
        "        if mask_token not in self.chars:\n",
        "            self.chars.append(mask_token)\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars, 1)}\n",
        "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.words[idx]\n",
        "        # masked_word = [self.char_to_idx[self.mask_token]] * len(word)\n",
        "        original_word = [self.char_to_idx[char] for char in word]\n",
        "        r = random.randint(1, len(word))\n",
        "        ind = sorted(set(random.sample(range(0,len(word)),r)))\n",
        "        word = list(word)\n",
        "        for i in range(len(word)):\n",
        "          if i in ind:\n",
        "            word[i]='_'\n",
        "        word = ''.join(word)\n",
        "        masked_word = [self.char_to_idx[char] for char in word]\n",
        "        return torch.tensor(masked_word), torch.tensor(original_word)\n",
        "\n",
        "# Custom collate function to pad sequences\n",
        "def collate_fn(batch, max_len=50):\n",
        "    masked_words, original_words = zip(*batch)\n",
        "    # max_len = max(len(word) for word in masked_words)\n",
        "    padded_masked_words = torch.zeros((len(masked_words), max_len), dtype=torch.long)\n",
        "    padded_original_words = torch.zeros((len(original_words), max_len), dtype=torch.long)\n",
        "\n",
        "\n",
        "    for i in range(len(masked_words)):\n",
        "        padded_masked_words[i, :len(masked_words[i])] = masked_words[i]\n",
        "        padded_original_words[i, :len(original_words[i])] = original_words[i]\n",
        "\n",
        "    return padded_masked_words, padded_original_words\n",
        "\n",
        "# Create dataset and dataloader with custom collate function\n",
        "dataset = CharDataset(words)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Define a Positional Encoding module6\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "# Define a simple Transformer model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward=64, max_len=34):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)\n",
        "        # src = self.pos_encoder(src)\n",
        "        output = self.transformer(src, src)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Model hyperparameters\n",
        "vocab_size = len(dataset.chars)+1\n",
        "d_model = 16\n",
        "nhead = 8\n",
        "num_encoder_layers = 1\n",
        "num_decoder_layers = 1\n",
        "max_len = 34\n",
        "\n",
        "# Instantiate the model\n",
        "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward=64, max_len=max_len)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(100):\n",
        "#     model.train()\n",
        "#     for masked_words, original_words in dataloader:\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Predict the whole sequence\n",
        "#         outputs = model(masked_words)\n",
        "\n",
        "#         # Compute loss for each position\n",
        "#         outputs = outputs.view(-1, vocab_size)\n",
        "#         original_words = original_words.view(-1)\n",
        "#         loss = criterion(outputs, original_words)\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "saved_model_path = '/content/newtry_149.pth'\n",
        "# # loaded_model_state_dict = torch.load(saved_model_path)\n",
        "\n",
        "# # Instantiate the model with the same architecture as before\n",
        "\n",
        "# torch.save(model.state_dict(), '/content/newtry_40.pth')\n",
        "path_to_model = '/content/newtry_149.pth'\n",
        "if not torch.cuda.is_available():\n",
        "  loaded_model_state_dict = torch.load(saved_model_path, map_location=torch.device('cpu'))\n",
        "else:\n",
        "    loaded_model_state_dict = torch.load(saved_model_path)\n",
        "\n",
        "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward=64, max_len=max_len)\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "model.load_state_dict(loaded_model_state_dict)\n",
        "\n",
        "\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "# model.load_state_dict(loaded_dict)\n",
        "# Testing the model (greedy decoding for simplicity)\n",
        "def decode(model, masked_word, max_len=10):\n",
        "    # model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            outputs = model(masked_word)\n",
        "            predictions = outputs.argmax(dim=-1)\n",
        "            probabilities = F.softmax(outputs, dim=-1)\n",
        "            # Update masked_word with new predictions\n",
        "            for i in range(masked_word.size(1)):\n",
        "                if masked_word[0, i] == dataset.char_to_idx['_']:\n",
        "                    masked_word[0, i] = predictions[1, i]\n",
        "                    break\n",
        "    return masked_word,probabilities\n",
        "\n",
        "# Example of decoding a fully masked word\n",
        "clean_word=\"ard_nt\"\n",
        "masked_word = torch.tensor([[dataset.char_to_idx[x]] for x in clean_word])\n",
        "decoded_word, probabilities = decode(model,masked_word)\n",
        "print([dataset.idx_to_char[int(i)] for i in decoded_word])\n",
        "len_word=len(clean_word)\n",
        "\n",
        "for i in range(len_word):\n",
        "  if clean_word[i] == \"_\":\n",
        "    max_prob_idx = int(np.argmax(probabilities[i]))\n",
        "    probs = probabilities[i].reshape([28])\n",
        "    indices = probs.argsort(descending=True)\n",
        "    dataset.idx_to_char[0]='?'\n",
        "    print([dataset.idx_to_char[int(i)] for i in indices])\n",
        "    # print(probabilities[i])\n",
        "    guessed_char = dataset.idx_to_char[max_prob_idx]\n",
        "    # print(guessed_char)\n",
        "    if guessed_char not in clean_word:\n",
        "      print(guessed_char)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqTJ0Yw8zAQ-",
        "outputId": "0c353732-9645-4bdf-f013-121a25c47a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'r', 'd', '_', 'n', 't']\n",
            "['r', 'n', 'a', 'e', 'l', 't', 'o', 'i', 's', 'c', 'p', 'm', 'u', 'd', 'b', 'g', 'h', 'f', 'y', 'v', 'w', 'k', 'x', 'z', 'j', 'q', '_', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of decoding a fully masked word\n",
        "clean_word=\"att_r_\"\n",
        "masked_word = torch.tensor([[dataset.char_to_idx[x]] for x in clean_word])\n",
        "decoded_word, probabilities = decode(model,masked_word)\n",
        "print([dataset.idx_to_char[int(i)] for i in decoded_word])\n",
        "len_word=len(clean_word)\n",
        "\n",
        "for i in range(len_word):\n",
        "  if clean_word[i] == \"_\":\n",
        "    max_prob_idx = int(np.argmax(probabilities[i]))\n",
        "    probs = probabilities[i].reshape([28])\n",
        "    indices = probs.argsort(descending=True)\n",
        "    dataset.idx_to_char[0]='?'\n",
        "    print([dataset.idx_to_char[int(i)] for i in indices])\n",
        "    # print(probabilities[i])\n",
        "    guessed_char = dataset.idx_to_char[max_prob_idx]\n",
        "    # print(guessed_char)\n",
        "    if guessed_char not in clean_word:\n",
        "      print(guessed_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-FNB2K5l5l7",
        "outputId": "6fe7ae65-b21a-49f3-9c5d-935823dad251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 't', 't', '_', 'r', '_']\n",
            "['r', 'e', 'a', 'n', 'l', 'o', 't', 'i', 's', 'c', 'p', 'm', 'u', 'd', 'g', 'b', 'h', 'f', 'y', 'v', 'w', 'k', 'x', 'z', 'j', 'q', '_', '?']\n",
            "['e', 'r', 'a', 'n', 't', 'i', 'l', 'o', 's', 'c', 'p', 'm', 'd', 'u', 'g', 'b', 'h', 'y', 'f', 'v', 'w', 'k', 'x', 'z', 'j', 'q', '_', '?']\n",
            "e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of decoding a fully masked word\n",
        "clean_word=\"att_re\"\n",
        "masked_word = torch.tensor([[dataset.char_to_idx[x]] for x in clean_word])\n",
        "decoded_word, probabilities = decode(model,masked_word)\n",
        "print([dataset.idx_to_char[int(i)] for i in decoded_word])\n",
        "len_word=len(clean_word)\n",
        "\n",
        "for i in range(len_word):\n",
        "  if clean_word[i] == \"_\":\n",
        "    max_prob_idx = int(np.argmax(probabilities[i]))\n",
        "    probs = probabilities[i].reshape([28])\n",
        "    indices = probs.argsort(descending=True)\n",
        "    dataset.idx_to_char[0]='?'\n",
        "    print([dataset.idx_to_char[int(i)] for i in indices])\n",
        "    # print(probabilities[i])\n",
        "    guessed_char = dataset.idx_to_char[max_prob_idx]\n",
        "    # print(guessed_char)\n",
        "    if guessed_char not in clean_word:\n",
        "      print(guessed_char)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHrYgAnymB5Y",
        "outputId": "51449729-afc4-4b15-9434-89eb06b04517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 't', 't', '_', 'r', 'e']\n",
            "['e', 'r', 'i', 'a', 't', 'o', 'l', 'n', 's', 'c', 'p', 'u', 'd', 'm', 'h', 'g', 'b', 'y', 'f', 'k', 'v', 'w', 'z', 'x', 'q', 'j', '_', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq= ['e', 't', 'a', 'o', 'i', 'n', 's', 'r', 'h', 'l', 'd', 'c', 'u', 'm', 'f', 'p', 'g', 'w', 'y', 'b', 'v', 'k', 'x', 'j', 'q', 'z']\n",
        "len(freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwCdiSaqRLB9",
        "outputId": "c0b26adf-6ab8-4742-d871-7485f1ff9262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}